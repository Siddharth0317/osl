{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siddharth0317/osl/blob/main/Exp_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bbzp7-09oqyd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text corpus\n",
        "corpus = \"\"\"Natural language processing enables computers to understand human language.\n",
        "Deep learning models like CBOW help us learn word representations.\n",
        "Word embeddings capture semantic meaning in vector space.\n",
        "This makes NLP applications like translation and sentiment analysis possible.\n",
        "The CBOW model predicts a word based on its context words.\"\"\"\n",
        "\n",
        "# Split into sentences and lowercase\n",
        "sentences = [s.strip().lower() for s in corpus.split('.') if s.strip()]\n",
        "\n",
        "# Tokenize text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Vocabulary and mapping\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "word_index = tokenizer.word_index\n",
        "index_word = {v: k for k, v in word_index.items()}\n",
        "index_word[0] = \"<PAD>\"\n",
        "\n",
        "print(\"Vocabulary:\", word_index)\n",
        "print(\"Sequences:\", sequences)\n",
        "print(\"Vocab Size:\", vocab_size)"
      ],
      "metadata": {
        "id": "BRC8ds1GpUIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Generate training data (context -> target)\n",
        "def generate_cbow_pairs(sequences, window_size=2):\n",
        "    contexts, targets = [], []\n",
        "    for seq in sequences:\n",
        "        for i, target in enumerate(seq):\n",
        "            context = []\n",
        "            for j in range(i - window_size, i + window_size + 1):\n",
        "                if j == i:\n",
        "                    continue\n",
        "                if 0 <= j < len(seq):\n",
        "                    context.append(seq[j])\n",
        "                else:\n",
        "                    context.append(0)  # padding\n",
        "            contexts.append(context)\n",
        "            targets.append(target)\n",
        "    return np.array(contexts), np.array(targets)\n",
        "\n",
        "# Generate dataset\n",
        "X, y = generate_cbow_pairs(sequences, window_size=2)\n",
        "\n",
        "print(\"Context shape:\", X.shape)\n",
        "print(\"Target shape:\", y.shape)\n",
        "print(\"Example:\")\n",
        "for i in range(5):\n",
        "    print([index_word[idx] for idx in X[i]], \"->\", index_word[y[i]])\n"
      ],
      "metadata": {
        "id": "JciINa7NpUGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Build and train CBOW model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Lambda, Dense, Input\n",
        "\n",
        "embedding_dim = 50\n",
        "context_len = X.shape[1]\n",
        "\n",
        "# Define CBOW model\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(context_len,)))\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
        "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))  # average embeddings\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, y, epochs=80, batch_size=16, verbose=2)\n"
      ],
      "metadata": {
        "id": "9KrP5LD5pUC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Output - Predictions and embeddings\n",
        "\n",
        "# Get learned embeddings\n",
        "embeddings = model.get_layer(\"embedding\").get_weights()[0]\n",
        "print(\"Embedding matrix shape:\", embeddings.shape)\n",
        "\n",
        "# Function to predict missing word from context\n",
        "def predict_word(context_words):\n",
        "    context_indices = [word_index.get(w, 0) for w in context_words]\n",
        "    context_indices = np.array(context_indices).reshape(1, -1)\n",
        "    probs = model.predict(context_indices, verbose=0)[0]\n",
        "    pred_idx = np.argmax(probs)\n",
        "    return index_word[pred_idx], float(probs[pred_idx])\n",
        "\n",
        "# Example prediction\n",
        "context = [\"deep\", \"models\", \"cbow\", \"help\"]  # must match context length = 4\n",
        "pred_word, prob = predict_word(context)\n",
        "print(\"Context:\", context)\n",
        "print(\"Predicted Word:\", pred_word, \"with probability:\", prob)\n",
        "\n",
        "# Function to find nearest words in embedding space\n",
        "def nearest_words(word, top_k=5):\n",
        "    if word not in word_index:\n",
        "        return []\n",
        "    w_idx = word_index[word]\n",
        "    vec = embeddings[w_idx]\n",
        "    norms = np.linalg.norm(embeddings, axis=1)\n",
        "    sims = embeddings.dot(vec) / (norms * np.linalg.norm(vec) + 1e-9)\n",
        "    top = np.argsort(-sims)[1: top_k+1]\n",
        "    return [(index_word[i], float(sims[i])) for i in top]\n",
        "\n",
        "print(\"Nearest words to 'learning':\", nearest_words(\"learning\"))\n"
      ],
      "metadata": {
        "id": "_2lzZgLhpT_r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}