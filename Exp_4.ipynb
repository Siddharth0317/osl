{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siddharth0317/osl/blob/main/Exp_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxVB6Gm3ufl1"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\n",
        "\n",
        "# Display plots inline\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (ensure GOOG.csv is uploaded to Colab)\n",
        "df = pd.read_csv('GOOG.csv')\n",
        "\n",
        "# Display first few rows\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "GALpHkQBuwFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract only 'Date' and 'Close' columns\n",
        "df = df[['Date', 'Close']]\n",
        "\n",
        "# Check info\n",
        "df.info()\n",
        "\n",
        "# Show date range\n",
        "print(df['Date'].min(), df['Date'].max())\n"
      ],
      "metadata": {
        "id": "dhr4Jh1LuwCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "train = df.loc[df['Date'] <= '2017-12-24']\n",
        "test  = df.loc[df['Date'] >  '2017-12-24']\n",
        "\n",
        "train.shape, test.shape\n"
      ],
      "metadata": {
        "id": "XO7ZnjWyuv_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale 'Close' column using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler = scaler.fit(np.array(train['Close']).reshape(-1, 1))\n",
        "\n",
        "train['Close'] = scaler.transform(np.array(train['Close']).reshape(-1, 1))\n",
        "test['Close']  = scaler.transform(np.array(test['Close']).reshape(-1, 1))\n",
        "\n",
        "# Visualize scaled training data\n",
        "plt.plot(train['Close'], label='Scaled Close Prices')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "m1KskS3Fuv8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TIME_STEPS = 30\n",
        "\n",
        "def create_sequences(X, y, time_steps=TIME_STEPS):\n",
        "    X_out, y_out = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        X_out.append(X.iloc[i:(i + time_steps)].values)\n",
        "        y_out.append(y.iloc[i + time_steps])\n",
        "    return np.array(X_out), np.array(y_out)\n",
        "\n",
        "# Create training and testing sequences\n",
        "X_train, y_train = create_sequences(train[['Close']], train['Close'])\n",
        "X_test, y_test   = create_sequences(test[['Close']], test['Close'])\n",
        "\n",
        "print(\"Training input shape:\", X_train.shape)\n",
        "print(\"Testing input shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "uUox3pRPuv25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "np.random.seed(21)\n",
        "tf.random.set_seed(21)\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    LSTM(128, activation='tanh', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    RepeatVector(X_train.shape[1]),\n",
        "    LSTM(128, activation='tanh', return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    TimeDistributed(Dense(X_train.shape[2]))\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "4yZebP_Eu5lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min')],\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Plot training vs validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F1BUcQyyu5jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on training data\n",
        "X_train_pred = model.predict(X_train)\n",
        "\n",
        "# Calculate MAE loss\n",
        "train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)\n",
        "\n",
        "# Visualize loss distribution\n",
        "plt.hist(train_mae_loss, bins=50)\n",
        "plt.xlabel('Train MAE loss')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()\n",
        "\n",
        "# Set reconstruction error threshold\n",
        "threshold = np.max(train_mae_loss)\n",
        "print('Reconstruction error threshold:', threshold)\n"
      ],
      "metadata": {
        "id": "rcHlUsRKu5gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "X_test_pred = model.predict(X_test, verbose=1)\n",
        "\n",
        "# Calculate test MAE loss\n",
        "test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
        "\n",
        "# Plot test loss distribution\n",
        "plt.hist(test_mae_loss, bins=50)\n",
        "plt.xlabel('Test MAE loss')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WyxUAQ1su5dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create anomaly dataframe\n",
        "anomaly_df = pd.DataFrame(test[TIME_STEPS:])\n",
        "anomaly_df['loss'] = test_mae_loss\n",
        "anomaly_df['threshold'] = threshold\n",
        "anomaly_df['anomaly'] = anomaly_df['loss'] > anomaly_df['threshold']\n",
        "\n",
        "# Extract anomalies\n",
        "anomalies = anomaly_df.loc[anomaly_df['anomaly'] == True]\n",
        "anomalies.head()\n"
      ],
      "metadata": {
        "id": "FReCL3VBvCWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot actual close prices and anomalies\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=anomaly_df['Date'],\n",
        "    y=scaler.inverse_transform(anomaly_df[['Close']]),\n",
        "    name='Close Price'\n",
        "))\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=anomalies['Date'],\n",
        "    y=scaler.inverse_transform(anomalies[['Close']]),\n",
        "    mode='markers',\n",
        "    name='Anomaly'\n",
        "))\n",
        "fig.update_layout(showlegend=True, title='Detected Anomalies in Google Stock Price')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "ZgcY6uHlvCTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "maqdBJSMvCP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}